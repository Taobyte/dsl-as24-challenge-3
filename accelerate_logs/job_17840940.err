The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `4`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2024-12-09 09:07:33.193497: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-12-09 09:07:33.193499: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-12-09 09:07:33.193499: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-12-09 09:07:33.193842: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1733731653.211897  988144 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1733731653.211897  988142 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1733731653.211896  988143 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1733731653.211896  988145 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1733731653.217469  988143 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1733731653.217470  988145 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1733731653.217469  988144 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1733731653.217469  988142 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-12-09 09:07:33.234929: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-12-09 09:07:33.234929: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-12-09 09:07:33.234929: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-12-09 09:07:33.234929: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Error executing job with overrides: []
Traceback (most recent call last):
  File "/cluster/scratch/ckeusch/dsl-as24-challenge-3/src/main.py", line 53, in main
    model = train_model(cfg)
            ^^^^^^^^^^^^^^^^
  File "/cluster/scratch/ckeusch/dsl-as24-challenge-3/src/train.py", line 25, in train_model
    model = fit_clean_unet_pytorch(cfg)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/scratch/ckeusch/dsl-as24-challenge-3/src/models/CleanUNet/train.py", line 319, in fit_clean_unet_pytorch
    accelerator = accelerate.Accelerator()
                  ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/ckeusch/miniconda3/envs/CDiffSD/lib/python3.11/site-packages/accelerate/accelerator.py", line 375, in __init__
    self.state = AcceleratorState(
                 ^^^^^^^^^^^^^^^^^
  File "/cluster/home/ckeusch/miniconda3/envs/CDiffSD/lib/python3.11/site-packages/accelerate/state.py", line 846, in __init__
    PartialState(cpu, **kwargs)
  File "/cluster/home/ckeusch/miniconda3/envs/CDiffSD/lib/python3.11/site-packages/accelerate/state.py", line 210, in __init__
    torch.distributed.init_process_group(backend=self.backend, **kwargs)
  File "/cluster/home/ckeusch/miniconda3/envs/CDiffSD/lib/python3.11/site-packages/torch/distributed/c10d_logger.py", line 86, in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/ckeusch/miniconda3/envs/CDiffSD/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py", line 1177, in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/ckeusch/miniconda3/envs/CDiffSD/lib/python3.11/site-packages/torch/distributed/rendezvous.py", line 246, in _env_rendezvous_handler
    store = _create_c10d_store(master_addr, master_port, rank, world_size, timeout, use_libuv)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/ckeusch/miniconda3/envs/CDiffSD/lib/python3.11/site-packages/torch/distributed/rendezvous.py", line 170, in _create_c10d_store
    tcp_store = TCPStore(hostname, port, world_size, False, timeout)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: random_device could not be read

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Error executing job with overrides: []
Error executing job with overrides: []
Traceback (most recent call last):
  File "/cluster/scratch/ckeusch/dsl-as24-challenge-3/src/main.py", line 53, in main
    model = train_model(cfg)
            ^^^^^^^^^^^^^^^^
  File "/cluster/scratch/ckeusch/dsl-as24-challenge-3/src/train.py", line 25, in train_model
    model = fit_clean_unet_pytorch(cfg)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/scratch/ckeusch/dsl-as24-challenge-3/src/models/CleanUNet/train.py", line 319, in fit_clean_unet_pytorch
    accelerator = accelerate.Accelerator()
                  ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/ckeusch/miniconda3/envs/CDiffSD/lib/python3.11/site-packages/accelerate/accelerator.py", line 375, in __init__
    self.state = AcceleratorState(
                 ^^^^^^^^^^^^^^^^^
  File "/cluster/home/ckeusch/miniconda3/envs/CDiffSD/lib/python3.11/site-packages/accelerate/state.py", line 846, in __init__
    PartialState(cpu, **kwargs)
  File "/cluster/home/ckeusch/miniconda3/envs/CDiffSD/lib/python3.11/site-packages/accelerate/state.py", line 210, in __init__
    torch.distributed.init_process_group(backend=self.backend, **kwargs)
  File "/cluster/home/ckeusch/miniconda3/envs/CDiffSD/lib/python3.11/site-packages/torch/distributed/c10d_logger.py", line 86, in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/ckeusch/miniconda3/envs/CDiffSD/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py", line 1177, in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/ckeusch/miniconda3/envs/CDiffSD/lib/python3.11/site-packages/torch/distributed/rendezvous.py", line 246, in _env_rendezvous_handler
    store = _create_c10d_store(master_addr, master_port, rank, world_size, timeout, use_libuv)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/ckeusch/miniconda3/envs/CDiffSD/lib/python3.11/site-packages/torch/distributed/rendezvous.py", line 170, in _create_c10d_store
    tcp_store = TCPStore(hostname, port, world_size, False, timeout)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: random_device could not be read

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Traceback (most recent call last):
  File "/cluster/scratch/ckeusch/dsl-as24-challenge-3/src/main.py", line 53, in main
    model = train_model(cfg)
            ^^^^^^^^^^^^^^^^
  File "/cluster/scratch/ckeusch/dsl-as24-challenge-3/src/train.py", line 25, in train_model
    model = fit_clean_unet_pytorch(cfg)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/scratch/ckeusch/dsl-as24-challenge-3/src/models/CleanUNet/train.py", line 319, in fit_clean_unet_pytorch
    accelerator = accelerate.Accelerator()
                  ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/ckeusch/miniconda3/envs/CDiffSD/lib/python3.11/site-packages/accelerate/accelerator.py", line 375, in __init__
    self.state = AcceleratorState(
                 ^^^^^^^^^^^^^^^^^
  File "/cluster/home/ckeusch/miniconda3/envs/CDiffSD/lib/python3.11/site-packages/accelerate/state.py", line 846, in __init__
    PartialState(cpu, **kwargs)
  File "/cluster/home/ckeusch/miniconda3/envs/CDiffSD/lib/python3.11/site-packages/accelerate/state.py", line 210, in __init__
    torch.distributed.init_process_group(backend=self.backend, **kwargs)
  File "/cluster/home/ckeusch/miniconda3/envs/CDiffSD/lib/python3.11/site-packages/torch/distributed/c10d_logger.py", line 86, in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/ckeusch/miniconda3/envs/CDiffSD/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py", line 1177, in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/ckeusch/miniconda3/envs/CDiffSD/lib/python3.11/site-packages/torch/distributed/rendezvous.py", line 246, in _env_rendezvous_handler
    store = _create_c10d_store(master_addr, master_port, rank, world_size, timeout, use_libuv)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/ckeusch/miniconda3/envs/CDiffSD/lib/python3.11/site-packages/torch/distributed/rendezvous.py", line 170, in _create_c10d_store
    tcp_store = TCPStore(hostname, port, world_size, False, timeout)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: random_device could not be read

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Error executing job with overrides: []
Traceback (most recent call last):
  File "/cluster/scratch/ckeusch/dsl-as24-challenge-3/src/main.py", line 53, in main
    model = train_model(cfg)
            ^^^^^^^^^^^^^^^^
  File "/cluster/scratch/ckeusch/dsl-as24-challenge-3/src/train.py", line 25, in train_model
    model = fit_clean_unet_pytorch(cfg)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/scratch/ckeusch/dsl-as24-challenge-3/src/models/CleanUNet/train.py", line 319, in fit_clean_unet_pytorch
    accelerator = accelerate.Accelerator()
                  ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/ckeusch/miniconda3/envs/CDiffSD/lib/python3.11/site-packages/accelerate/accelerator.py", line 375, in __init__
    self.state = AcceleratorState(
                 ^^^^^^^^^^^^^^^^^
  File "/cluster/home/ckeusch/miniconda3/envs/CDiffSD/lib/python3.11/site-packages/accelerate/state.py", line 846, in __init__
    PartialState(cpu, **kwargs)
  File "/cluster/home/ckeusch/miniconda3/envs/CDiffSD/lib/python3.11/site-packages/accelerate/state.py", line 210, in __init__
    torch.distributed.init_process_group(backend=self.backend, **kwargs)
  File "/cluster/home/ckeusch/miniconda3/envs/CDiffSD/lib/python3.11/site-packages/torch/distributed/c10d_logger.py", line 86, in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/ckeusch/miniconda3/envs/CDiffSD/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py", line 1177, in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/ckeusch/miniconda3/envs/CDiffSD/lib/python3.11/site-packages/torch/distributed/rendezvous.py", line 246, in _env_rendezvous_handler
    store = _create_c10d_store(master_addr, master_port, rank, world_size, timeout, use_libuv)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/ckeusch/miniconda3/envs/CDiffSD/lib/python3.11/site-packages/torch/distributed/rendezvous.py", line 170, in _create_c10d_store
    tcp_store = TCPStore(hostname, port, world_size, False, timeout)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: random_device could not be read

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
[2024-12-09 09:08:12,363] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 988142) of binary: /cluster/home/ckeusch/miniconda3/envs/CDiffSD/bin/python3.11
Traceback (most recent call last):
  File "/cluster/home/ckeusch/miniconda3/envs/CDiffSD/bin/accelerate", line 10, in <module>
    sys.exit(main())
             ^^^^^^
  File "/cluster/home/ckeusch/miniconda3/envs/CDiffSD/lib/python3.11/site-packages/accelerate/commands/accelerate_cli.py", line 46, in main
    args.func(args)
  File "/cluster/home/ckeusch/miniconda3/envs/CDiffSD/lib/python3.11/site-packages/accelerate/commands/launch.py", line 1066, in launch_command
    multi_gpu_launcher(args)
  File "/cluster/home/ckeusch/miniconda3/envs/CDiffSD/lib/python3.11/site-packages/accelerate/commands/launch.py", line 711, in multi_gpu_launcher
    distrib_run.run(args)
  File "/cluster/home/ckeusch/miniconda3/envs/CDiffSD/lib/python3.11/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/cluster/home/ckeusch/miniconda3/envs/CDiffSD/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/ckeusch/miniconda3/envs/CDiffSD/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
src/main.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-12-09_09:08:12
  host      : eu-g3-077.euler.ethz.ch
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 988143)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-12-09_09:08:12
  host      : eu-g3-077.euler.ethz.ch
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 988144)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2024-12-09_09:08:12
  host      : eu-g3-077.euler.ethz.ch
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 988145)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-12-09_09:08:12
  host      : eu-g3-077.euler.ethz.ch
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 988142)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
